{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d5fc15",
   "metadata": {},
   "source": [
    "# Face image generation with StyleGAN\n",
    "\n",
    "- https://keras.io/examples/generative/stylegan/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64ef982",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The key idea of StyleGAN is to progressively increase the resolution of the generated images and to incorporate style features in the generative process.This StyleGAN implementation is based on the book Hands-on Image Generation with TensorFlow. The code from the book's Github repository was refactored to leverage a custom train_step() to enable faster training time via compilation and distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736ef971",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install latest TFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c93a075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.6.2)\n",
      "Collecting tensorflow_addons\n",
      "  Downloading tensorflow_addons-0.14.0-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.3.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.19.5)\n",
      "Collecting gdown\n",
      "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.13.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorboard<2.7,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.39.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: keras<2.7,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (5.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.6/dist-packages (from gdown) (2.26.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.4.1-py3-none-any.whl (9.9 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.64.0)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 86.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (57.4.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (1.34.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (0.4.5)\n",
      "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (2.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown) (2.0.4)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6; extra == \"socks\"\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tqdm->gdown) (5.4.0)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow) (0.8)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (4.6.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.7\"->tqdm->gdown) (3.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow) (3.1.1)\n",
      "Building wheels for collected packages: gdown\n",
      "  Building wheel for gdown (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14775 sha256=67936a57dbf0be3d0e5de0adb0a8f5aa61558f41615bdae894c3bde6d11973f0\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/f9/d3/4594b3b2fe2ee239d0c9eb861f468204652e0a1c6d03755d75\n",
      "Successfully built gdown\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: typeguard, tensorflow-addons, filelock, soupsieve, beautifulsoup4, gdown, PySocks\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.11.1 filelock-3.4.1 gdown-4.4.0 soupsieve-2.3.2.post1 tensorflow-addons-0.14.0 typeguard-2.13.3\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow tensorflow_addons matplotlib numpy gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9af24d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from enum import Enum\n",
    "from glob import glob\n",
    "from functools import partial\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "\n",
    "import gdown\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e9e3e",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "In this example, we will train using the CelebA from TensorFlow Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b5a6633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 202599 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "def log2(x):\n",
    "    return int(np.log2(x))\n",
    "\n",
    "\n",
    "# we use different batch size for different resolution, so larger image size\n",
    "# could fit into GPU memory. The keys is image resolution in log2\n",
    "batch_sizes = {2: 16, 3: 16, 4: 16, 5: 16, 6: 16, 7: 8, 8: 4, 9: 2, 10: 1}\n",
    "# We adjust the train step accordingly\n",
    "train_step_ratio = {k: batch_sizes[2] / v for k, v in batch_sizes.items()}\n",
    "\n",
    "if not os.path.exists(\"celeba_gan\"):\n",
    "    os.makedirs(\"celeba_gan\")\n",
    "\n",
    "url = \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\"\n",
    "output = \"celeba_gan/data.zip\"\n",
    "gdown.download(url, output, quiet=True)\n",
    "\n",
    "with ZipFile(\"celeba_gan/data.zip\", \"r\") as zipobj:\n",
    "    zipobj.extractall(\"celeba_gan\")\n",
    "\n",
    "# Create a dataset from our folder, and rescale the images to the [0-1] range:\n",
    "\n",
    "ds_train = keras.preprocessing.image_dataset_from_directory(\n",
    "    \"celeba_gan\", label_mode=None, image_size=(64, 64), batch_size=32\n",
    ")\n",
    "ds_train = ds_train.map(lambda x: x / 255.0)\n",
    "\n",
    "\n",
    "def resize_image(res, image):\n",
    "    # only donwsampling, so use nearest neighbor that is faster to run\n",
    "    image = tf.image.resize(\n",
    "        image, (res, res), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
    "    )\n",
    "    image = tf.cast(image, tf.float32) / 127.5 - 1.0\n",
    "    return image\n",
    "\n",
    "\n",
    "def create_dataloader(res):\n",
    "    batch_size = batch_sizes[log2(res)]\n",
    "    # NOTE: we unbatch the dataset so we can `batch()` it again with the `drop_remainder=True` option\n",
    "    # since the model only supports a single batch size\n",
    "    dl = ds_train.map(partial(resize_image, res), num_parallel_calls=tf.data.AUTOTUNE).unbatch()\n",
    "    dl = dl.shuffle(200).batch(batch_size, drop_remainder=True).prefetch(1).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45de8dd3",
   "metadata": {},
   "source": [
    "## Utility function to display images after each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc6d8412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, log2_res, fname=\"\"):\n",
    "    scales = {2: 0.5, 3: 1, 4: 2, 5: 3, 6: 4, 7: 5, 8: 6, 9: 7, 10: 8}\n",
    "    scale = scales[log2_res]\n",
    "\n",
    "    grid_col = min(images.shape[0], int(32 // scale))\n",
    "    grid_row = 1\n",
    "\n",
    "    f, axarr = plt.subplots(\n",
    "        grid_row, grid_col, figsize=(grid_col * scale, grid_row * scale)\n",
    "    )\n",
    "\n",
    "    for row in range(grid_row):\n",
    "        ax = axarr if grid_row == 1 else axarr[row]\n",
    "        for col in range(grid_col):\n",
    "            ax[col].imshow(images[row * grid_col + col])\n",
    "            ax[col].axis(\"off\")\n",
    "    plt.show()\n",
    "    if fname:\n",
    "        f.savefig(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff9671",
   "metadata": {},
   "source": [
    "## Custom Layers\n",
    "The following are building blocks that will be used to construct the generators and discriminators of the StyleGAN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3e2c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fade_in(alpha, a, b):\n",
    "    return alpha * a + (1.0 - alpha) * b\n",
    "\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return -tf.reduce_mean(y_true * y_pred)\n",
    "\n",
    "\n",
    "def pixel_norm(x, epsilon=1e-8):\n",
    "    return x / tf.math.sqrt(tf.reduce_mean(x ** 2, axis=-1, keepdims=True) + epsilon)\n",
    "\n",
    "\n",
    "def minibatch_std(input_tensor, epsilon=1e-8):\n",
    "    n, h, w, c = tf.shape(input_tensor)\n",
    "    group_size = tf.minimum(4, n)\n",
    "    x = tf.reshape(input_tensor, [group_size, -1, h, w, c])\n",
    "    group_mean, group_var = tf.nn.moments(x, axes=(0), keepdims=False)\n",
    "    group_std = tf.sqrt(group_var + epsilon)\n",
    "    avg_std = tf.reduce_mean(group_std, axis=[1, 2, 3], keepdims=True)\n",
    "    x = tf.tile(avg_std, [group_size, h, w, 1])\n",
    "    return tf.concat([input_tensor, x], axis=-1)\n",
    "\n",
    "\n",
    "class EqualizedConv(layers.Layer):\n",
    "    def __init__(self, out_channels, kernel=3, gain=2, **kwargs):\n",
    "        super(EqualizedConv, self).__init__(**kwargs)\n",
    "        self.kernel = kernel\n",
    "        self.out_channels = out_channels\n",
    "        self.gain = gain\n",
    "        self.pad = kernel != 1\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.in_channels = input_shape[-1]\n",
    "        initializer = keras.initializers.RandomNormal(mean=0.0, stddev=1.0)\n",
    "        self.w = self.add_weight(\n",
    "            shape=[self.kernel, self.kernel, self.in_channels, self.out_channels],\n",
    "            initializer=initializer,\n",
    "            trainable=True,\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.out_channels,), initializer=\"zeros\", trainable=True, name=\"bias\"\n",
    "        )\n",
    "        fan_in = self.kernel * self.kernel * self.in_channels\n",
    "        self.scale = tf.sqrt(self.gain / fan_in)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.pad:\n",
    "            x = tf.pad(inputs, [[0, 0], [1, 1], [1, 1], [0, 0]], mode=\"REFLECT\")\n",
    "        else:\n",
    "            x = inputs\n",
    "        output = (\n",
    "            tf.nn.conv2d(x, self.scale * self.w, strides=1, padding=\"VALID\") + self.b\n",
    "        )\n",
    "        return output\n",
    "\n",
    "\n",
    "class EqualizedDense(layers.Layer):\n",
    "    def __init__(self, units, gain=2, learning_rate_multiplier=1, **kwargs):\n",
    "        super(EqualizedDense, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.gain = gain\n",
    "        self.learning_rate_multiplier = learning_rate_multiplier\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.in_channels = input_shape[-1]\n",
    "        initializer = keras.initializers.RandomNormal(\n",
    "            mean=0.0, stddev=1.0 / self.learning_rate_multiplier\n",
    "        )\n",
    "        self.w = self.add_weight(\n",
    "            shape=[self.in_channels, self.units],\n",
    "            initializer=initializer,\n",
    "            trainable=True,\n",
    "            name=\"kernel\",\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"zeros\", trainable=True, name=\"bias\"\n",
    "        )\n",
    "        fan_in = self.in_channels\n",
    "        self.scale = tf.sqrt(self.gain / fan_in)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output = tf.add(tf.matmul(inputs, self.scale * self.w), self.b)\n",
    "        return output * self.learning_rate_multiplier\n",
    "\n",
    "\n",
    "class AddNoise(layers.Layer):\n",
    "    def build(self, input_shape):\n",
    "        n, h, w, c = input_shape[0]\n",
    "        initializer = keras.initializers.RandomNormal(mean=0.0, stddev=1.0)\n",
    "        self.b = self.add_weight(\n",
    "            shape=[1, 1, 1, c], initializer=initializer, trainable=True, name=\"kernel\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, noise = inputs\n",
    "        output = x + self.b * noise\n",
    "        return output\n",
    "\n",
    "\n",
    "class AdaIN(layers.Layer):\n",
    "    def __init__(self, gain=1, **kwargs):\n",
    "        super(AdaIN, self).__init__(**kwargs)\n",
    "        self.gain = gain\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        x_shape = input_shapes[0]\n",
    "        w_shape = input_shapes[1]\n",
    "\n",
    "        self.w_channels = w_shape[-1]\n",
    "        self.x_channels = x_shape[-1]\n",
    "\n",
    "        self.dense_1 = EqualizedDense(self.x_channels, gain=1)\n",
    "        self.dense_2 = EqualizedDense(self.x_channels, gain=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, w = inputs\n",
    "        ys = tf.reshape(self.dense_1(w), (-1, 1, 1, self.x_channels))\n",
    "        yb = tf.reshape(self.dense_2(w), (-1, 1, 1, self.x_channels))\n",
    "        return ys * x + yb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdebe2b",
   "metadata": {},
   "source": [
    "Next we build the following:\n",
    "\n",
    "- A model mapping to map the random noise into style code\n",
    "- The generator\n",
    "- The discriminator\n",
    "\n",
    "For the generator, we build generator blocks at multiple resolutions, e.g. 4x4, 8x8, ...up to 1024x1024. We only use 4x4 in the beginning and we use progressively larger-resolution blocks as the training proceeds. Same for the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b24d0e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mapping(num_stages, input_shape=512):\n",
    "    z = layers.Input(shape=(input_shape))\n",
    "    w = pixel_norm(z)\n",
    "    for i in range(8):\n",
    "        w = EqualizedDense(512, learning_rate_multiplier=0.01)(w)\n",
    "        w = layers.LeakyReLU(0.2)(w)\n",
    "    w = tf.tile(tf.expand_dims(w, 1), (1, num_stages, 1))\n",
    "    return keras.Model(z, w, name=\"mapping\")\n",
    "\n",
    "\n",
    "class Generator:\n",
    "    def __init__(self, start_res_log2, target_res_log2):\n",
    "        self.start_res_log2 = start_res_log2\n",
    "        self.target_res_log2 = target_res_log2\n",
    "        self.num_stages = target_res_log2 - start_res_log2 + 1\n",
    "        # list of generator blocks at increasing resolution\n",
    "        self.g_blocks = []\n",
    "        # list of layers to convert g_block activation to RGB\n",
    "        self.to_rgb = []\n",
    "        # list of noise input of different resolutions into g_blocks\n",
    "        self.noise_inputs = []\n",
    "        # filter size to use at each stage, keys are log2(resolution)\n",
    "        self.filter_nums = {\n",
    "            0: 512,\n",
    "            1: 512,\n",
    "            2: 512,  # 4x4\n",
    "            3: 512,  # 8x8\n",
    "            4: 512,  # 16x16\n",
    "            5: 512,  # 32x32\n",
    "            6: 256,  # 64x64\n",
    "            7: 128,  # 128x128\n",
    "            8: 64,  # 256x256\n",
    "            9: 32,  # 512x512\n",
    "            10: 16,\n",
    "        }  # 1024x1024\n",
    "\n",
    "        start_res = 2 ** start_res_log2\n",
    "        self.input_shape = (start_res, start_res, self.filter_nums[start_res_log2])\n",
    "        self.g_input = layers.Input(self.input_shape, name=\"generator_input\")\n",
    "\n",
    "        for i in range(start_res_log2, target_res_log2 + 1):\n",
    "            filter_num = self.filter_nums[i]\n",
    "            res = 2 ** i\n",
    "            self.noise_inputs.append(\n",
    "                layers.Input(shape=(res, res, 1), name=f\"noise_{res}x{res}\")\n",
    "            )\n",
    "            to_rgb = Sequential(\n",
    "                [\n",
    "                    layers.InputLayer(input_shape=(res, res, filter_num)),\n",
    "                    EqualizedConv(3, 1, gain=1),\n",
    "                ],\n",
    "                name=f\"to_rgb_{res}x{res}\",\n",
    "            )\n",
    "            self.to_rgb.append(to_rgb)\n",
    "            is_base = i == self.start_res_log2\n",
    "            if is_base:\n",
    "                input_shape = (res, res, self.filter_nums[i - 1])\n",
    "            else:\n",
    "                input_shape = (2 ** (i - 1), 2 ** (i - 1), self.filter_nums[i - 1])\n",
    "            g_block = self.build_block(\n",
    "                filter_num, res=res, input_shape=input_shape, is_base=is_base\n",
    "            )\n",
    "            self.g_blocks.append(g_block)\n",
    "\n",
    "    def build_block(self, filter_num, res, input_shape, is_base):\n",
    "        input_tensor = layers.Input(shape=input_shape, name=f\"g_{res}\")\n",
    "        noise = layers.Input(shape=(res, res, 1), name=f\"noise_{res}\")\n",
    "        w = layers.Input(shape=512)\n",
    "        x = input_tensor\n",
    "\n",
    "        if not is_base:\n",
    "            x = layers.UpSampling2D((2, 2))(x)\n",
    "            x = EqualizedConv(filter_num, 3)(x)\n",
    "\n",
    "        x = AddNoise()([x, noise])\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = AdaIN()([x, w])\n",
    "\n",
    "        x = EqualizedConv(filter_num, 3)(x)\n",
    "        x = AddNoise()([x, noise])\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = AdaIN()([x, w])\n",
    "        return keras.Model([input_tensor, w, noise], x, name=f\"genblock_{res}x{res}\")\n",
    "\n",
    "    def grow(self, res_log2):\n",
    "        res = 2 ** res_log2\n",
    "\n",
    "        num_stages = res_log2 - self.start_res_log2 + 1\n",
    "        w = layers.Input(shape=(self.num_stages, 512), name=\"w\")\n",
    "\n",
    "        alpha = layers.Input(shape=(1), name=\"g_alpha\")\n",
    "        x = self.g_blocks[0]([self.g_input, w[:, 0], self.noise_inputs[0]])\n",
    "\n",
    "        if num_stages == 1:\n",
    "            rgb = self.to_rgb[0](x)\n",
    "        else:\n",
    "            for i in range(1, num_stages - 1):\n",
    "\n",
    "                x = self.g_blocks[i]([x, w[:, i], self.noise_inputs[i]])\n",
    "\n",
    "            old_rgb = self.to_rgb[num_stages - 2](x)\n",
    "            old_rgb = layers.UpSampling2D((2, 2))(old_rgb)\n",
    "\n",
    "            i = num_stages - 1\n",
    "            x = self.g_blocks[i]([x, w[:, i], self.noise_inputs[i]])\n",
    "\n",
    "            new_rgb = self.to_rgb[i](x)\n",
    "\n",
    "            rgb = fade_in(alpha[0], new_rgb, old_rgb)\n",
    "\n",
    "        return keras.Model(\n",
    "            [self.g_input, w, self.noise_inputs, alpha],\n",
    "            rgb,\n",
    "            name=f\"generator_{res}_x_{res}\",\n",
    "        )\n",
    "\n",
    "\n",
    "class Discriminator:\n",
    "    def __init__(self, start_res_log2, target_res_log2):\n",
    "        self.start_res_log2 = start_res_log2\n",
    "        self.target_res_log2 = target_res_log2\n",
    "        self.num_stages = target_res_log2 - start_res_log2 + 1\n",
    "        # filter size to use at each stage, keys are log2(resolution)\n",
    "        self.filter_nums = {\n",
    "            0: 512,\n",
    "            1: 512,\n",
    "            2: 512,  # 4x4\n",
    "            3: 512,  # 8x8\n",
    "            4: 512,  # 16x16\n",
    "            5: 512,  # 32x32\n",
    "            6: 256,  # 64x64\n",
    "            7: 128,  # 128x128\n",
    "            8: 64,  # 256x256\n",
    "            9: 32,  # 512x512\n",
    "            10: 16,\n",
    "        }  # 1024x1024\n",
    "        # list of discriminator blocks at increasing resolution\n",
    "        self.d_blocks = []\n",
    "        # list of layers to convert RGB into activation for d_blocks inputs\n",
    "        self.from_rgb = []\n",
    "\n",
    "        for res_log2 in range(self.start_res_log2, self.target_res_log2 + 1):\n",
    "            res = 2 ** res_log2\n",
    "            filter_num = self.filter_nums[res_log2]\n",
    "            from_rgb = Sequential(\n",
    "                [\n",
    "                    layers.InputLayer(\n",
    "                        input_shape=(res, res, 3), name=f\"from_rgb_input_{res}\"\n",
    "                    ),\n",
    "                    EqualizedConv(filter_num, 1),\n",
    "                    layers.LeakyReLU(0.2),\n",
    "                ],\n",
    "                name=f\"from_rgb_{res}\",\n",
    "            )\n",
    "\n",
    "            self.from_rgb.append(from_rgb)\n",
    "\n",
    "            input_shape = (res, res, filter_num)\n",
    "            if len(self.d_blocks) == 0:\n",
    "                d_block = self.build_base(filter_num, res)\n",
    "            else:\n",
    "                d_block = self.build_block(\n",
    "                    filter_num, self.filter_nums[res_log2 - 1], res\n",
    "                )\n",
    "\n",
    "            self.d_blocks.append(d_block)\n",
    "\n",
    "    def build_base(self, filter_num, res):\n",
    "        input_tensor = layers.Input(shape=(res, res, filter_num), name=f\"d_{res}\")\n",
    "        x = minibatch_std(input_tensor)\n",
    "        x = EqualizedConv(filter_num, 3)(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = EqualizedDense(filter_num)(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = EqualizedDense(1)(x)\n",
    "        return keras.Model(input_tensor, x, name=f\"d_{res}\")\n",
    "\n",
    "    def build_block(self, filter_num_1, filter_num_2, res):\n",
    "        input_tensor = layers.Input(shape=(res, res, filter_num_1), name=f\"d_{res}\")\n",
    "        x = EqualizedConv(filter_num_1, 3)(input_tensor)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = EqualizedConv(filter_num_2)(x)\n",
    "        x = layers.LeakyReLU(0.2)(x)\n",
    "        x = layers.AveragePooling2D((2, 2))(x)\n",
    "        return keras.Model(input_tensor, x, name=f\"d_{res}\")\n",
    "\n",
    "    def grow(self, res_log2):\n",
    "        res = 2 ** res_log2\n",
    "        idx = res_log2 - self.start_res_log2\n",
    "        alpha = layers.Input(shape=(1), name=\"d_alpha\")\n",
    "        input_image = layers.Input(shape=(res, res, 3), name=\"input_image\")\n",
    "        x = self.from_rgb[idx](input_image)\n",
    "        x = self.d_blocks[idx](x)\n",
    "        if idx > 0:\n",
    "            idx -= 1\n",
    "            downsized_image = layers.AveragePooling2D((2, 2))(input_image)\n",
    "            y = self.from_rgb[idx](downsized_image)\n",
    "            x = fade_in(alpha[0], x, y)\n",
    "\n",
    "            for i in range(idx, -1, -1):\n",
    "                x = self.d_blocks[i](x)\n",
    "        return keras.Model([input_image, alpha], x, name=f\"discriminator_{res}_x_{res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97b7b0a",
   "metadata": {},
   "source": [
    "## Build StyleGAN with custom train step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bba0432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleGAN(tf.keras.Model):\n",
    "    def __init__(self, z_dim=512, target_res=64, start_res=4):\n",
    "        super(StyleGAN, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.target_res_log2 = log2(target_res)\n",
    "        self.start_res_log2 = log2(start_res)\n",
    "        self.current_res_log2 = self.target_res_log2\n",
    "        self.num_stages = self.target_res_log2 - self.start_res_log2 + 1\n",
    "\n",
    "        self.alpha = tf.Variable(1.0, dtype=tf.float32, trainable=False, name=\"alpha\")\n",
    "\n",
    "        self.mapping = Mapping(num_stages=self.num_stages)\n",
    "        self.d_builder = Discriminator(self.start_res_log2, self.target_res_log2)\n",
    "        self.g_builder = Generator(self.start_res_log2, self.target_res_log2)\n",
    "        self.g_input_shape = self.g_builder.input_shape\n",
    "\n",
    "        self.phase = None\n",
    "        self.train_step_counter = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
    "\n",
    "        self.loss_weights = {\"gradient_penalty\": 10, \"drift\": 0.001}\n",
    "\n",
    "    def grow_model(self, res):\n",
    "        tf.keras.backend.clear_session()\n",
    "        res_log2 = log2(res)\n",
    "        self.generator = self.g_builder.grow(res_log2)\n",
    "        self.discriminator = self.d_builder.grow(res_log2)\n",
    "        self.current_res_log2 = res_log2\n",
    "        print(f\"\\nModel resolution:{res}x{res}\")\n",
    "\n",
    "    def compile(\n",
    "        self, steps_per_epoch, phase, res, d_optimizer, g_optimizer, *args, **kwargs\n",
    "    ):\n",
    "        self.loss_weights = kwargs.pop(\"loss_weights\", self.loss_weights)\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        if res != 2 ** self.current_res_log2:\n",
    "            self.grow_model(res)\n",
    "            self.d_optimizer = d_optimizer\n",
    "            self.g_optimizer = g_optimizer\n",
    "\n",
    "        self.train_step_counter.assign(0)\n",
    "        self.phase = phase\n",
    "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
    "        super(StyleGAN, self).compile(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "\n",
    "    def generate_noise(self, batch_size):\n",
    "        noise = [\n",
    "            tf.random.normal((batch_size, 2 ** res, 2 ** res, 1))\n",
    "            for res in range(self.start_res_log2, self.target_res_log2 + 1)\n",
    "        ]\n",
    "        return noise\n",
    "\n",
    "    def gradient_loss(self, grad):\n",
    "        loss = tf.square(grad)\n",
    "        loss = tf.reduce_sum(loss, axis=tf.range(1, tf.size(tf.shape(loss))))\n",
    "        loss = tf.sqrt(loss)\n",
    "        loss = tf.reduce_mean(tf.square(loss - 1))\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "\n",
    "        self.train_step_counter.assign_add(1)\n",
    "\n",
    "        if self.phase == \"TRANSITION\":\n",
    "            self.alpha.assign(\n",
    "                tf.cast(self.train_step_counter / self.steps_per_epoch, tf.float32)\n",
    "            )\n",
    "        elif self.phase == \"STABLE\":\n",
    "            self.alpha.assign(1.0)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        alpha = tf.expand_dims(self.alpha, 0)\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        real_labels = tf.ones(batch_size)\n",
    "        fake_labels = -tf.ones(batch_size)\n",
    "\n",
    "        z = tf.random.normal((batch_size, self.z_dim))\n",
    "        const_input = tf.ones(tuple([batch_size] + list(self.g_input_shape)))\n",
    "        noise = self.generate_noise(batch_size)\n",
    "\n",
    "        # generator\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            w = self.mapping(z)\n",
    "            fake_images = self.generator([const_input, w, noise, alpha])\n",
    "            pred_fake = self.discriminator([fake_images, alpha])\n",
    "            g_loss = wasserstein_loss(real_labels, pred_fake)\n",
    "\n",
    "            trainable_weights = (\n",
    "                self.mapping.trainable_weights + self.generator.trainable_weights\n",
    "            )\n",
    "            gradients = g_tape.gradient(g_loss, trainable_weights)\n",
    "            self.g_optimizer.apply_gradients(zip(gradients, trainable_weights))\n",
    "\n",
    "        # discriminator\n",
    "        with tf.GradientTape() as gradient_tape, tf.GradientTape() as total_tape:\n",
    "            # forward pass\n",
    "            pred_fake = self.discriminator([fake_images, alpha])\n",
    "            pred_real = self.discriminator([real_images, alpha])\n",
    "\n",
    "            epsilon = tf.random.uniform((batch_size, 1, 1, 1))\n",
    "            interpolates = epsilon * real_images + (1 - epsilon) * fake_images\n",
    "            gradient_tape.watch(interpolates)\n",
    "            pred_fake_grad = self.discriminator([interpolates, alpha])\n",
    "\n",
    "            # calculate losses\n",
    "            loss_fake = wasserstein_loss(fake_labels, pred_fake)\n",
    "            loss_real = wasserstein_loss(real_labels, pred_real)\n",
    "            loss_fake_grad = wasserstein_loss(fake_labels, pred_fake_grad)\n",
    "\n",
    "            # gradient penalty\n",
    "            gradients_fake = gradient_tape.gradient(loss_fake_grad, [interpolates])\n",
    "            gradient_penalty = self.loss_weights[\n",
    "                \"gradient_penalty\"\n",
    "            ] * self.gradient_loss(gradients_fake)\n",
    "\n",
    "            # drift loss\n",
    "            all_pred = tf.concat([pred_fake, pred_real], axis=0)\n",
    "            drift_loss = self.loss_weights[\"drift\"] * tf.reduce_mean(all_pred ** 2)\n",
    "\n",
    "            d_loss = loss_fake + loss_real + gradient_penalty + drift_loss\n",
    "\n",
    "            gradients = total_tape.gradient(\n",
    "                d_loss, self.discriminator.trainable_weights\n",
    "            )\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(gradients, self.discriminator.trainable_weights)\n",
    "            )\n",
    "\n",
    "        # Update metrics\n",
    "        self.d_loss_metric.update_state(d_loss)\n",
    "        self.g_loss_metric.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_metric.result(),\n",
    "            \"g_loss\": self.g_loss_metric.result(),\n",
    "        }\n",
    "\n",
    "    def call(self, inputs: dict()):\n",
    "        style_code = inputs.get(\"style_code\", None)\n",
    "        z = inputs.get(\"z\", None)\n",
    "        noise = inputs.get(\"noise\", None)\n",
    "        batch_size = inputs.get(\"batch_size\", 1)\n",
    "        alpha = inputs.get(\"alpha\", 1.0)\n",
    "        alpha = tf.expand_dims(alpha, 0)\n",
    "        if style_code is None:\n",
    "            if z is None:\n",
    "                z = tf.random.normal((batch_size, self.z_dim))\n",
    "            style_code = self.mapping(z)\n",
    "\n",
    "        if noise is None:\n",
    "            noise = self.generate_noise(batch_size)\n",
    "\n",
    "        # self.alpha.assign(alpha)\n",
    "\n",
    "        const_input = tf.ones(tuple([batch_size] + list(self.g_input_shape)))\n",
    "        images = self.generator([const_input, style_code, noise, alpha])\n",
    "        images = np.clip((images * 0.5 + 0.5) * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "        return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214c496b",
   "metadata": {},
   "source": [
    "## Training\n",
    "We first build the StyleGAN at smallest resolution, such as 4x4 or 8x8. Then we progressively grow the model to higher resolution by appending new generator and discriminator blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbaa3492",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_RES = 4\n",
    "TARGET_RES = 128\n",
    "\n",
    "style_gan = StyleGAN(start_res=START_RES, target_res=TARGET_RES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535d553a",
   "metadata": {},
   "source": [
    "The training for each new resolution happen in two phases - \"transition\" and \"stable\". In the transition phase, the features from the previous resolution are mixed with the current resolution. This allows for a smoother transition when scalling up. We use each epoch in model.fit() as a phase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "478006fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    start_res=START_RES,\n",
    "    target_res=TARGET_RES,\n",
    "    steps_per_epoch=5000,\n",
    "    display_images=True,\n",
    "):\n",
    "    opt_cfg = {\"learning_rate\": 1e-3, \"beta_1\": 0.0, \"beta_2\": 0.99, \"epsilon\": 1e-8}\n",
    "\n",
    "    val_batch_size = 16\n",
    "    val_z = tf.random.normal((val_batch_size, style_gan.z_dim))\n",
    "    val_noise = style_gan.generate_noise(val_batch_size)\n",
    "\n",
    "    start_res_log2 = int(np.log2(start_res))\n",
    "    target_res_log2 = int(np.log2(target_res))\n",
    "\n",
    "    for res_log2 in range(start_res_log2, target_res_log2 + 1):\n",
    "        res = 2 ** res_log2\n",
    "        for phase in [\"TRANSITION\", \"STABLE\"]:\n",
    "            if res == start_res and phase == \"TRANSITION\":\n",
    "                continue\n",
    "\n",
    "            train_dl = create_dataloader(res)\n",
    "\n",
    "            steps = int(train_step_ratio[res_log2] * steps_per_epoch)\n",
    "\n",
    "            style_gan.compile(\n",
    "                d_optimizer=tf.keras.optimizers.Adam(**opt_cfg),\n",
    "                g_optimizer=tf.keras.optimizers.Adam(**opt_cfg),\n",
    "                loss_weights={\"gradient_penalty\": 10, \"drift\": 0.001},\n",
    "                steps_per_epoch=steps,\n",
    "                res=res,\n",
    "                phase=phase,\n",
    "                run_eagerly=False,\n",
    "            )\n",
    "\n",
    "            prefix = f\"res_{res}x{res}_{style_gan.phase}\"\n",
    "\n",
    "            ckpt_cb = keras.callbacks.ModelCheckpoint(\n",
    "                f\"checkpoints/stylegan_{res}x{res}.ckpt\",\n",
    "                save_weights_only=True,\n",
    "                verbose=0,\n",
    "            )\n",
    "            print(phase)\n",
    "            style_gan.fit(\n",
    "                train_dl, epochs=1, steps_per_epoch=steps, callbacks=[ckpt_cb]\n",
    "            )\n",
    "\n",
    "            if display_images:\n",
    "                images = style_gan({\"z\": val_z, \"noise\": val_noise, \"alpha\": 1.0})\n",
    "                plot_images(images, res_log2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28265d09",
   "metadata": {},
   "source": [
    "StyleGAN can take a long time to train, in the code below, a small steps_per_epoch value of 1 is used to sanity-check the code is working alright. In practice, a larger steps_per_epoch value (over 10000) is required to get decent results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b886dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model resolution:4x4\n",
      "STABLE\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'NoneType'>, <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f2130cba25ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_res\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_res\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-0c4a41ce27e9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(start_res, target_res, steps_per_epoch, display_images)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             style_gan.fit(\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mckpt_cb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             )\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1146\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1135\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1138\u001b[0m     self._adapter = adapter_cls(\n\u001b[1;32m   1139\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    977\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \"input: {}, {}\".format(\n\u001b[0;32m--> 979\u001b[0;31m             _type_name(x), _type_name(y)))\n\u001b[0m\u001b[1;32m    980\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'NoneType'>, <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "train(start_res=4, target_res=16, steps_per_epoch=1, display_images=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393405de",
   "metadata": {},
   "source": [
    "## Results\n",
    "We can now run some inference using pre-trained 64x64 checkpoints. In general, the image fidelity increases with the resolution. You can try to train this StyleGAN to resolutions above 128x128 with the CelebA HQ dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b9caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/soon-yau/stylegan_keras/releases/download/keras_example_v1.0/stylegan_128x128.ckpt.zip\"\n",
    "\n",
    "weights_path = keras.utils.get_file(\n",
    "    \"stylegan_128x128.ckpt.zip\",\n",
    "    url,\n",
    "    extract=True,\n",
    "    cache_dir=os.path.abspath(\".\"),\n",
    "    cache_subdir=\"pretrained\",\n",
    ")\n",
    "\n",
    "style_gan.grow_model(128)\n",
    "style_gan.load_weights(os.path.join(\"pretrained/stylegan_128x128.ckpt\"))\n",
    "\n",
    "tf.random.set_seed(196)\n",
    "batch_size = 2\n",
    "z = tf.random.normal((batch_size, style_gan.z_dim))\n",
    "w = style_gan.mapping(z)\n",
    "noise = style_gan.generate_noise(batch_size=batch_size)\n",
    "images = style_gan({\"style_code\": w, \"noise\": noise, \"alpha\": 1.0})\n",
    "plot_images(images, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803fed52",
   "metadata": {},
   "source": [
    "## Style Mixing\n",
    "We can also mix styles from two images to create a new image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11848fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.4\n",
    "w_mix = np.expand_dims(alpha * w[0] + (1 - alpha) * w[1], 0)\n",
    "noise_a = [np.expand_dims(n[0], 0) for n in noise]\n",
    "mix_images = style_gan({\"style_code\": w_mix, \"noise\": noise_a})\n",
    "image_row = np.hstack([images[0], images[1], mix_images[0]])\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.imshow(image_row)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbd496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
